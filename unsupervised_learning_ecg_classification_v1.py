# -*- coding: utf-8 -*-
"""Unsupervised learning - ECG Classification_V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eiOt7pov98_yilye6JmFS2MEHyGRvAAb

**This notebook involves the clustering of heart beats. We will use K-Means, & Gaussian mixture models for clustering.**

### import required libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import preprocessing
from sklearn.cluster import KMeans, AffinityPropagation
from scipy.cluster.hierarchy import fcluster, ward, dendrogram

import warnings
warnings.filterwarnings('ignore')

"""### Load the datasets"""

df_train = pd.read_csv('/content/drive/MyDrive/MIT-BIH Arrhythmia Database /mitbih_train.csv', header=None)
df_test = pd.read_csv('/content/drive/MyDrive/MIT-BIH Arrhythmia Database /mitbih_test.csv', header=None)

df_train.head(100)

df_train.shape

df_test.head()

df_test.shape

"""#### Let's see how our target variable distributed in our training data."""

#classes count
df_class=df_train[187].value_counts()
print('Count of classes :\n',df_class)
#Percentage of classes count
per_class=df_class/len(df_train)*100
print('percentage of count of classes :\n',per_class)

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df_train[187],palette='husl')
plt.title('Distribution of train class labels')
plt.xlabel('label')
plt.ylabel('Index')

"""#### Let's see how our target variable distributed in our testing data."""

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df_test[187],palette='husl')
plt.title('Distribution of test class labels')
plt.xlabel('label')
plt.ylabel('Index')

# Convert target data type from float to int
df_train[187] = df_train[187].astype('uint8')

df_train

df_train[187].unique()

# Consider 2000 observations for each class in train data, --> 2000*5 = 10000 observations.

df_class1 = df_train[df_train[187]==0].reset_index(drop=True).loc[:2000, :]
df_class2 = df_train[df_train[187]==1].reset_index(drop=True).loc[:2000, :]
df_class3 = df_train[df_train[187]==2].reset_index(drop=True).loc[:2000, :]
df_class4 = df_train[df_train[187]==3].reset_index(drop=True).loc[:2000, :]
df_class5 = df_train[df_train[187]==4].reset_index(drop=True).loc[:2000, :]

# Concat class 1 & class 2
df_train_final = pd.concat([df_class1, df_class2], axis=0).reset_index(drop=True)

# Concat df_final & class 3
df_train_final = pd.concat([df_train_final, df_class3], axis=0).reset_index(drop=True)

# Concat df_final & class 4
df_train_final = pd.concat([df_train_final, df_class4], axis=0).reset_index(drop=True)

# Concat df_final & class 5
df_train_final = pd.concat([df_train_final, df_class5], axis=0).reset_index(drop=True)

df_train_final

"""### Create features & label set"""

# Features
x_train = df_train_final.drop(187, axis=1)
x_test = df_test.drop(187, axis=1)

# Labels
y_train = df_train_final[187]
y_test = df_test[187]

x_train

x_test

y_train

y_test

"""The labels column  has 5 classes – one class is the normal heart signal and other three are different signals corresponding to different heart diseases and last category is unknown beats:

0 — Non-ectopic beats (normal beat)      
1 — Supraventricular ectopic beats        
2 — Ventricular ectopic beats      
3 — Fusion beats          
4 — Unknown beats     

['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]
"""

class_names = ['N','S','V','F','Q']

def apply(row):
    if row == 0:
        return 'N'
    elif row==1:
        return 'S'
    elif row ==2:
        return 'V'
    elif row ==3:
        return 'F'
    else:
        return 'Q'

# Apply above function

y_train_labels = y_train.apply(lambda x : apply(x))
y_test_labels = y_test.apply(lambda x : apply(x))

y_test_labels

y_train_labels

"""The class 3 has low number of observations compared to other classes. So, SMOTE is used to balance all classes."""

from imblearn.over_sampling import SMOTE
from collections import Counter

counter= Counter(y_train)
print(f'Before Sampling', counter)
# Oversampling the train data using SMOTE
smt=SMOTE()
x_train_sm,y_train_sm = smt.fit_resample(x_train, y_train)
counter= Counter(y_train_sm)
print('After Sampling', counter)

"""### Data pre-processing

Check if any missing values
"""

df_train.isnull().sum()

"""It seems our dataset doesn't have missing values.

### Normalize the data
"""

from sklearn.preprocessing import  QuantileTransformer

# Initialize Quantile Transformer
scaler =  QuantileTransformer()

import numpy as np
from sklearn.preprocessing import quantile_transform
rng = np.random.RandomState(0)
X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)
quantile_transform(X, n_quantiles=10, random_state=0, copy=True)

X

"""## **Build Clustering models**

We are considering 2 algorithms for clustering, viz
* K-Means
* Gaussian mixture model

> **K-Means**:  
K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. For this algorithm, we need to select optimum number of clusters that separates the data well.

> **Gaussian mixture model**:      
 Gaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster. Hence, a Gaussian Mixture Model tends to group the data points belonging to a single distribution together. Gaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters.

For more details, https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/

### K-Means Clustering

Since the **number of target classes is 5**, we assume the **number of clusters (K) as 5**.
"""

# Initialize the K-Means
kmeans_model  = KMeans(n_clusters = 5, random_state=42)
# Fit model
kmeans_model.fit(x_train)

"""### Predict clusters on test data"""

y_pred_cluster = kmeans_model.predict(x_test.values)

y_pred_cluster

"""### Distribution of classes in test data"""

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(y_test, palette='husl')
plt.title('Distribution of actual class labels')
plt.xlabel('label')
plt.ylabel('Index')

"""### Distribution of K-Means clusters predicted in test data"""

df_kmeans =x_test.copy()
df_kmeans['predicted_cluster'] = y_pred_cluster

df_kmeans

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df_kmeans.predicted_cluster, palette='husl')
plt.title('Distribution of predicted cluster labels')
plt.xlabel('label')
plt.ylabel('Index')

"""Insights:
* Most of the observations grouped are in cluster 0.
* Cluster 2 has least number of observations.
"""

from sklearn.decomposition import PCA

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D

# Define a function to plot 3D highlighting
def Axes3d_plot(df, method):
    # Initialize PCA
    pca = PCA(n_components=3)  # Reduce to k=3 dimensions
    # Fit
    df_pca = pca.fit_transform(df.drop('predicted_cluster', axis=1))

    df_pca = pd.DataFrame(df_pca, columns = ['X_component','Y_component','Z_component'])
    df_pca['predicted_cluster'] = df.predicted_cluster

    cluster_1 = df_pca[df_pca.predicted_cluster==0]
    cluster_2 = df_pca[df_pca.predicted_cluster==1]
    cluster_3 = df_pca[df_pca.predicted_cluster==2]
    cluster_4 = df_pca[df_pca.predicted_cluster==3]
    cluster_5 = df_pca[df_pca.predicted_cluster==4]

    # Plot PCA components
    fig = plt.figure(figsize = (12, 6))
    ax = fig.add_subplot(111, projection='3d')
    ax.set_title(f'PCA analysis - {method}')
    ax.set_xlabel("X component")
    ax.set_ylabel("Y component")
    ax.set_zlabel("Z component")
    # Plot cluster 1
    ax.scatter(cluster_1['X_component'], cluster_1['Y_component'], zs=cluster_1['Z_component'], s=4, lw=2, label="0", c="green")
    ax.scatter(cluster_2['X_component'], cluster_2['Y_component'], zs=cluster_2['Z_component'], s=4, lw=2, label="1", c="cyan")
    ax.scatter(cluster_3['X_component'], cluster_3['Y_component'], zs=cluster_3['Z_component'], s=4, lw=2, label="2", c="yellow")
    ax.scatter(cluster_4['X_component'], cluster_4['Y_component'], zs=cluster_4['Z_component'], s=4, lw=2, label="3", c="orange")
    ax.scatter(cluster_5['X_component'], cluster_5['Y_component'], zs=cluster_5['Z_component'], s=4, lw=2, label="4", c="m")

    ax.legend()
    plt.show()

# Plot 3D for K-Means
Axes3d_plot(df_kmeans, 'K-Means Clustering')

"""It seems some observations are clustered wrongly."""

# Add actual labels
df_kmeans['Actual_labels'] = y_test.astype('uint8')
df_kmeans['Actual_label_names'] = y_test_labels

# Create a dataframe for each cluster
df1_kmeans = df_kmeans[df_kmeans['predicted_cluster']==0].reset_index(drop=True)
df2_kmeans = df_kmeans[df_kmeans['predicted_cluster']==1].reset_index(drop=True)
df3_kmeans = df_kmeans[df_kmeans['predicted_cluster']==2].reset_index(drop=True)
df4_kmeans = df_kmeans[df_kmeans['predicted_cluster']==3].reset_index(drop=True)
df5_kmeans = df_kmeans[df_kmeans['predicted_cluster']==4].reset_index(drop=True)

df1_kmeans.head()

df2_kmeans.head()

df3_kmeans.head()

df4_kmeans.head()

df5_kmeans.head()

kmeans_df = df_kmeans.set_index("predicted_cluster Actual_label_names".split()).sort_index()
kmeans_df

# Actual_labels
kmeans_df = kmeans_df.drop('Actual_labels', axis=1)

kmeans_df

"""### How heart signals distributed in each cluster?

### Cluster 1
"""

# Apply PCA
pca = PCA(n_components=2, random_state=42)
df_pca1  = pca.fit_transform(df1_kmeans.drop(['predicted_cluster', 'Actual_label_names', 'Actual_labels'], axis=1))
# Create a dataframe for df_pca1
df_pca1 = pd.DataFrame(df_pca1, columns = ['X_component', 'Y_component'])
df_pca1['predicted_cluster'] = df1_kmeans.predicted_cluster
df_pca1['Actual_labels'] = df1_kmeans.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df1_kmeans.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(df_pca1['X_component'], df_pca1['Y_component'], hue = df_pca1['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 1')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 1 contains most of the observations belonging to the class 0.

### Cluster 2
"""

df_pca2  = pca.fit_transform(df2_kmeans.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca2
df_pca2 = pd.DataFrame(df_pca2, columns = ['X_component', 'Y_component'])
df_pca2['predicted_cluster'] = df2_kmeans.predicted_cluster
df_pca2['Actual_labels'] = df2_kmeans.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df2_kmeans.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(df_pca2['X_component'], df_pca2['Y_component'], hue = df_pca2['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 2')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 2 contains most of the observations belonging to the class 0, & 4.

### Cluster 3
"""

df_pca3  = pca.fit_transform(df3_kmeans.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca3
df_pca3 = pd.DataFrame(df_pca3, columns = ['X_component', 'Y_component'])
df_pca3['predicted_cluster'] = df3_kmeans.predicted_cluster
df_pca3['Actual_labels'] = df3_kmeans.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df3_kmeans.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(df_pca3['X_component'], df_pca3['Y_component'], hue = df_pca3['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 3')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 3 contains most of the observations belonging to the class 0, & 2.

### Cluster 4
"""

df_pca4  = pca.fit_transform(df4_kmeans.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca4
df_pca4 = pd.DataFrame(df_pca4, columns = ['X_component', 'Y_component'])
df_pca4['predicted_cluster'] = df4_kmeans.predicted_cluster
df_pca4['Actual_labels'] = df4_kmeans.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df4_kmeans.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(df_pca4['X_component'], df_pca4['Y_component'], hue = df_pca4['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 3')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 4 contains most of the observations belonging to the class 0.

### Cluster 5
"""

df_pca5  = pca.fit_transform(df5_kmeans.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca5
df_pca5 = pd.DataFrame(df_pca5, columns = ['X_component', 'Y_component'])
df_pca5['predicted_cluster'] = df5_kmeans.predicted_cluster
df_pca5['Actual_labels'] = df5_kmeans.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(df5_kmeans.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(df_pca5['X_component'], df_pca5['Y_component'], hue = df_pca5['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 5')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 5 contains most of the observations belonging to the class 0 followed by 2.

### Let's analyze 100 random observations from each cluster to understand the patterns in it.
"""

import random

# Function to plot clusters
def plot_100samples(label,df , bbox, ncol):
    fig=df.loc[label].T.iloc[:, random.sample(range(df.loc[label].shape[0]), 100)].plot(figsize = (12,6),\
                                       title=f'Number of observations in cluster {label+1} : {len(df.loc[label])}')\
    .legend(title='Actual class names', bbox_to_anchor=bbox, ncol=ncol)
    return print(fig)

#Plot for Cluster 1
plot_100samples(0, kmeans_df, (1.02, 1.02), 4)
#Plot for Cluster 2
plot_100samples(1,kmeans_df, (1.02, 1.02), 4)
#Plot for Cluster 3
plot_100samples(2, kmeans_df, (1.02, 1.02), 4)
#Plot for Cluster 4
plot_100samples(3, kmeans_df, (1.02, 1.02), 4)
#Plot for Cluster 5
plot_100samples(4, kmeans_df, (1.02, 1.02), 4)

"""### Evaluate K-means model

We are using Fowlkes-Mallows index (FMI)/score for evaluating K-means & GMM models.

It measure the similarity of two clusterings of a set of points. The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:       
FMI = TP / sqrt((TP + FP) * (TP + FN))

Where TP is the number of True Positive (i.e. the number of pair of points that belongs in the same clusters in both labels_true and labels_pred), FP is the number of False Positive (i.e. the number of pair of points that belongs in the same clusters in labels_true and not in labels_pred) and FN is the number of False Negative (i.e the number of pair of points that belongs in the same clusters in labels_pred and not in labels_True).
"""

# Evaluate K-means model
from sklearn.metrics.cluster import fowlkes_mallows_score

fow_ma_score = fowlkes_mallows_score(y_test, df_kmeans['predicted_cluster'])
fow_ma_score

"""## Gaussian Mixture model"""

# training gaussian mixture model
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=5, covariance_type = 'spherical', random_state=42)
gmm.fit(x_train)

"""### Predict clusters on test data"""

y_pred_clust = gmm.predict(x_test.values)

# Evalauate GMM model
fow_ma_score = fowlkes_mallows_score(y_test, y_pred_clust)
fow_ma_score

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(y_pred_clust, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('label')
plt.ylabel('Index')

"""Insights:
* We found most of the observations grouped together in cluster 4.
* Cluster 0 has least number of observations.
"""

gmm_model = x_test.copy()
gmm_model['predicted_cluster'] = y_pred_clust

# Plot 3D for Gaussin Mixture model
Axes3d_plot(gmm_model, 'GMM Clustering')

# Add labels
gmm_model['Actual_labels'] = y_test.astype('uint8')
gmm_model['Actual_label_names'] = y_test_labels

"""### GMM Clustering analysis"""

# Create a dataframe for each cluster
df1_gmm = gmm_model[gmm_model['predicted_cluster']==0].reset_index(drop=True)
df2_gmm = gmm_model[gmm_model['predicted_cluster']==1].reset_index(drop=True)
df3_gmm = gmm_model[gmm_model['predicted_cluster']==2].reset_index(drop=True)
df4_gmm = gmm_model[gmm_model['predicted_cluster']==3].reset_index(drop=True)
df5_gmm = gmm_model[gmm_model['predicted_cluster']==4].reset_index(drop=True)

df1_gmm.head()

df2_gmm.head()

df3_gmm.head()

df4_gmm.head()

df5_gmm.head()

"""### Cluster 1"""

gmm_pca1  = pca.fit_transform(df1_gmm.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca1
gmm_pca1 = pd.DataFrame(gmm_pca1, columns = ['X_component', 'Y_component'])
gmm_pca1['predicted_cluster'] = df1_gmm.predicted_cluster
gmm_pca1['Actual_labels'] = df1_gmm.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(gmm_pca1.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(gmm_pca1['X_component'], gmm_pca1['Y_component'], hue = gmm_pca1['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 1')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 1 contains most of the observations belonging to the class 0.

### Cluster 2
"""

gmm_pca2  = pca.fit_transform(df2_kmeans.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca2
gmm_pca2 = pd.DataFrame(gmm_pca2, columns = ['X_component', 'Y_component'])
gmm_pca2['predicted_cluster'] = df2_gmm.predicted_cluster
gmm_pca2['Actual_labels'] = df2_gmm.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(gmm_pca2.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(gmm_pca2['X_component'], gmm_pca2['Y_component'], hue = gmm_pca2['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 2')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* All of the observations from class 0 in cluster 2.

### Cluster 3
"""

gmm_pca3  = pca.fit_transform(df3_gmm.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names' ], axis=1))
# Create a dataframe for df_pca3
gmm_pca3 = pd.DataFrame(gmm_pca3, columns = ['X_component', 'Y_component'])
gmm_pca3['predicted_cluster'] = df3_gmm.predicted_cluster
gmm_pca3['Actual_labels'] = df3_gmm.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(gmm_pca3.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(gmm_pca3['X_component'], gmm_pca3['Y_component'], hue = gmm_pca3['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 3')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Most of the observations are from class 0 & 4 in cluster 3.

### Cluster 4
"""

gmm_pca4  = pca.fit_transform(df4_gmm.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca4
gmm_pca4 = pd.DataFrame(gmm_pca4, columns = ['X_component', 'Y_component'])
gmm_pca4['predicted_cluster'] = df4_gmm.predicted_cluster
gmm_pca4['Actual_labels'] = df4_gmm.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(gmm_pca4.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(gmm_pca4['X_component'], gmm_pca4['Y_component'], hue = gmm_pca4['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 4')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Cluster 4 contains most number of observations from class 0 & 2.

### Cluster 5
"""

gmm_pca5  = pca.fit_transform(df5_gmm.drop(['predicted_cluster', 'Actual_labels', 'Actual_label_names'], axis=1))
# Create a dataframe for df_pca5
gmm_pca5 = pd.DataFrame(gmm_pca5, columns = ['X_component', 'Y_component'])
gmm_pca5['predicted_cluster'] = df5_gmm.predicted_cluster
gmm_pca5['Actual_labels'] = df5_gmm.Actual_labels

#Countplot
plt.figure(figsize=(12,6))
sns.countplot(gmm_pca5.Actual_labels, palette='husl')
plt.title('Distribution of predicted class labels')
plt.xlabel('Label')
plt.ylabel('Index')

# Scatter plot
plt.figure(figsize=(12,6))
sns.scatterplot(gmm_pca5['X_component'], gmm_pca5['Y_component'], hue = gmm_pca5['Actual_labels'], palette='tab10')
plt.title('Scatter plot : Cluster 5')
plt.xlabel('X_component')
plt.ylabel('Y_component')

"""Insights:
* Most of the observations are from class 0 in cluster 5.

### Let's analyze 100 random observations from each cluster to understand the patterns in it.
"""

gmm_df = gmm_model.set_index("predicted_cluster Actual_label_names".split()).sort_index()
gmm_df

gmm_df = gmm_df.drop('Actual_labels', axis=1)

#Plot for Cluster 1
plot_100samples(0, gmm_df, (1.02, 1.02), 4)
#Plot for Cluster 2
plot_100samples(1, gmm_df, (1.02, 1.02), 4)
#Plot for Cluster 3
plot_100samples(2, gmm_df, (1.02, 1.02), 4)
#Plot for Cluster 4
plot_100samples(3, gmm_df, (1.02, 1.02), 4)
#Plot for Cluster 5
plot_100samples(4, gmm_df, (1.02, 1.02), 4)

"""Final insights:
* fowlkes_mallows_score is 0.73 for K-means, & 0.72 for gmm.
* If you look at the patterns, we can say that both methods showing unique patterns in each cluster.
* From 3D plots, we can say that, clusters generated from GMM are well separated compared to K-means.
* We found some wrongly clustered observations in most of the clusters for both methods. In most of the clusters the normal beats class is present, it seems normal beats signals have some co-relation with other signals.

Note:
* This method will not applicable if we don't know the ground truth/actual labels.
"""

